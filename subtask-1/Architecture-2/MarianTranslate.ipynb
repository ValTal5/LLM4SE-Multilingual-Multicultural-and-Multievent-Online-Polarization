{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "100326bc",
      "metadata": {},
      "source": [
        "# Subtasks 1 & 2: Translation with MarianTranslate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d81edd",
      "metadata": {
        "id": "90d81edd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "class Translator:\n",
        "    def __init__(self, source_lang: str, device: str):\n",
        "        self.device = device\n",
        "        self.source_lang = source_lang\n",
        "        model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-en\"\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "            self.model = MarianMTModel.from_pretrained(model_name).to(self.device)\n",
        "        except Exception as e:\n",
        "            raise(f\"Error during the download of the model for the language {source_lang}: {e}\")\n",
        "\n",
        "    def translate_to_eng(self, text: str) -> str:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "        translated = self.model.generate(**inputs)\n",
        "        tgt_text = self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "        return tgt_text\n",
        "\n",
        "    def translate_batch_to_eng(self, texts: list[str]) -> list[str]:\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            translated = self.model.generate(**inputs)\n",
        "\n",
        "        return [\n",
        "            self.tokenizer.decode(t, skip_special_tokens=True)\n",
        "            for t in translated\n",
        "        ]\n",
        "\n",
        "\n",
        "    def translate_directory(self, input_dir, output_dir, text_column=\"text\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        for filename in os.listdir(input_dir):\n",
        "            if not filename.endswith(\".csv\"):\n",
        "                continue\n",
        "\n",
        "            in_path = os.path.join(input_dir, filename)\n",
        "            out_path = os.path.join(output_dir, f\"en_{filename}\")\n",
        "\n",
        "            print(f\"Translating {filename}\")\n",
        "\n",
        "            with open(in_path, encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                rows = list(reader)\n",
        "                fieldnames = reader.fieldnames + [\"text_en\"]\n",
        "\n",
        "            for row in tqdm(rows):\n",
        "                row[\"text_en\"] = self.translate_to_eng(row[text_column])\n",
        "\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(rows)\n",
        "\n",
        "    def translate_file_by_batch(self, input_file_path, output_dir, batch_size=8, text_column=\"text\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        input_path = Path(input_file_path)\n",
        "\n",
        "        if not input_path.exists():\n",
        "            raise (\"The file doesn't exist\")\n",
        "\n",
        "        if not input_path.suffix == \".csv\":\n",
        "            raise (\"The extension file is not correct\")\n",
        "\n",
        "        filename = input_path.name\n",
        "\n",
        "        out_path = os.path.join(output_dir, f\"en_{filename}\")\n",
        "\n",
        "        print(f\"Translating {filename} from {self.source_lang} into eng...\")\n",
        "\n",
        "        with open(input_file_path, encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            rows = list(reader)\n",
        "            fieldnames = reader.fieldnames + [\"text_en\"]\n",
        "\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for i in tqdm(range(0, len(rows), batch_size)):\n",
        "                batch_rows = rows[i : i + batch_size]\n",
        "\n",
        "                texts = [row.get(text_column, \"\") for row in batch_rows]\n",
        "\n",
        "                translations = self.translate_batch_to_eng(texts)\n",
        "\n",
        "                for row, text_en in zip(batch_rows, translations):\n",
        "                    row[\"text_en\"] = text_en if text_en else \"\"\n",
        "\n",
        "                writer.writerows(batch_rows)\n",
        "                f.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptr4o3a1XLdM",
      "metadata": {
        "id": "ptr4o3a1XLdM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "source_langs = [\"ar\", \"bn\", \"de\", \"hi\", \"it\", \"pl\", \"ru\", \"es\", \"tr\", \"ur\", \"zh\"]\n",
        "file_prefixes = [\"arb\", \"ben\", \"deu\", \"hin\", \"ita\", \"pol\", \"rus\", \"spa\", \"tur\", \"urd\", \"zho\"]\n",
        "output_dir = \"./translation\"\n",
        "\n",
        "for lang, prefix in zip(source_langs, file_prefixes):\n",
        "    print(f\"Processing language: {lang} with file: {prefix}.csv\")\n",
        "\n",
        "    try:\n",
        "        translator = Translator(lang, device)\n",
        "\n",
        "        input_file = f\"./train/{prefix}.csv\"\n",
        "\n",
        "        translator.translate_file_by_batch(input_file, output_dir, batch_size=32)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {lang} ({prefix}.csv): {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
